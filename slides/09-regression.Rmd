---
title: "Regression and practical advice"
author: "Toby Dylan Hocking"
output: 
  beamer_presentation:
    dev: png
    includes:
      in_header: "04-header.tex"	
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  dpi=200,
  fig.width=10,
  fig.height=6)
possible.conda <- c(
  "~/.local/share/r-miniconda",
  "~/miniconda3")
conda.dir <- normalizePath(possible.conda[dir.exists(possible.conda)][1])
env.name <- "2023-08-deep-learning"
python <- if(.Platform$OS.type=="unix"){
  "bin/python"
}else{
  "python.exe"
}
Sys.setenv(RETICULATE_PYTHON=paste0(
  conda.dir, "/envs/", env.name, "/", python))
reticulate::use_condaenv(env.name, required=TRUE)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
if(FALSE){
  system("R -e \"rmarkdown::render('09-regression.Rmd')\"")
}
```

# Supervised machine learning

- Goal is to learn a function $f(\mathbf x)=y$ where $\mathbf
  x\in\mathbb R^p$ is an input/feature vector and $y$ is an
  output/label.
- This week we will study linear models and neural networks for
  regression, meaning labels represented by $y\in \mathbb R$ is a real
  number.
- air foil self-noise data: $\mathbf x=$ Frequency (Hertz), Angle of
  attack (degrees), Chord length (meters), Free-stream velocity
  (meters per second), $y\in\mathbb R$ Scaled sound pressure level, in
  decibels.
- forest fires data: $\mathbf x=$meteorological and other data, 
  $y\in\mathbb R_+$ burned area.
- some practical advice for getting gradient descent learning to work
  better (scaling, log transform, feature transform)
  
---

# air foil self-noise data 

```{python results=TRUE}
import plotnine as p9
p9.options.figure_size=(4.5,3)
import numpy as np
width = 55
np.set_printoptions(linewidth=width)
import pandas as pd
#https://pandas.pydata.org/docs/user_guide/options.html
import os
import re
# grid/contouring functions
def make_grid(mat, n_grid = 80):
    nrow, ncol = mat.shape
    assert ncol == 2
    mesh_args = mat.apply(
        lambda x: np.linspace(min(x),max(x), n_grid), axis=0)
    mesh_tup = np.meshgrid(*[mesh_args[x] for x in mesh_args])
    mesh_vectors = [v.flatten() for v in mesh_tup]
    out_df = pd.DataFrame(dict(zip(mesh_args,mesh_vectors)))
    out_df.columns = mat.columns
    return out_df
# https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.find_contours
# https://scikit-image.org/docs/dev/auto_examples/edges/plot_contours.html#sphx-glr-auto-examples-edges-plot-contours-py
def contour_paths(three_cols, level):
    from skimage import measure
    uniq_df = three_cols.iloc[:,:2].apply(pd.unique)
    n_grid = uniq_df.shape[0]
    fun_mat = three_cols.iloc[:,2].to_numpy().reshape(
        [n_grid,n_grid]).transpose()
    contours = measure.find_contours(fun_mat, level)
    contour_df_list = []
    half_df = (uniq_df-uniq_df.diff()/2)[1:]
    half_df.index = [x-0.5 for x in half_df.index]
    lookup_df = pd.concat([uniq_df, half_df])
    for contour_i, contour_mat in enumerate(contours):
        one_contour_df = pd.DataFrame(contour_mat)
        one_contour_df.columns = [c+"_i" for c in uniq_df]
        one_contour_df["contour_i"] = contour_i
        for cname in lookup_df:
            iname = cname+"_i"
            contour_col = one_contour_df[iname]
            lookup_col = lookup_df[cname]
            index_df = lookup_col[contour_col].reset_index()
            one_contour_df[cname] = index_df[cname]
        contour_df_list.append(one_contour_df)
    return pd.concat(contour_df_list)

# work-around for rendering plots under windows, which hangs within
# emacs python shell: instead write a PNG file and view in browser.
import webbrowser
on_windows = os.name == "nt"
in_render = r.in_render if 'r' in dir() else False
using_agg = on_windows and not in_render
if using_agg:
    import matplotlib
    matplotlib.use("agg")
def show(g):
    if not using_agg:
        return g
    g.save("tmp.png")
    webbrowser.open(os.getcwd()+'/tmp.png')

data_info_dict = {
    "forest_fires":("../data/forestfires.csv",","),
    "air_foil":("../data/airfoil_self_noise.tsv","\t"),
}
def norm01(label_vec):
    lmin = label_vec.min()
    return (label_vec-lmin)/(label_vec.max()-lmin)
data_dict = {}
hist_df_list = []
for data_name,(file_name,sep) in data_info_dict.items():
    data_df = pd.read_csv(file_name,sep=sep,header=0)
    data_df.columns = [
        re.sub(".*[.]", "",col) if "." in col else col
        for col in data_df.columns]
    data_dict[data_name] = data_df
    data_nrow, data_ncol = data_df.shape
    label_col_num = data_ncol-1
    data_label_vec = data_df.iloc[:,label_col_num]
    def append(transform, label_vec):
        hist_df_list.append(pd.DataFrame({
            "data_name":data_name,
            "transform":transform,
            "label":label_vec,
            "label_name":data_df.columns[label_col_num]
        }))
    append("none",data_label_vec)
    append("norm01",norm01(data_label_vec))
    log_label_vec = np.log(data_label_vec-data_label_vec.min()+1)
    append("log",log_label_vec)
    append("log,norm01",norm01(log_label_vec))
    is_feature_col = (
        np.arange(data_ncol) != label_col_num
    ) & (
        data_df.dtypes != "object"
    )
    data_features = data_df.loc[:,is_feature_col]
    feature_nrow, feature_ncol= data_features.shape
    feature_mean = data_features.mean().to_numpy().reshape(1,feature_ncol)
    feature_std = data_features.std().to_numpy().reshape(1,feature_ncol)
    feature_scaled = (data_features-feature_mean)/feature_std
hist_df = pd.concat(hist_df_list)
pd.set_option("display.max_columns", 4)
data_dict["air_foil"]
```

Need to scale label vector, to avoid numerical instability in gradient
descent.

---

```{python}
#p9.theme(text=p9.element_text(size=30))+\
def show_data_trans(data_name, transform, exclude_zero=False):
    some_hist_df = hist_df.query(
        f'data_name=="{data_name}" & transform=="{transform}"')
    tit = f"Labels in data={data_name}"
    if exclude_zero:
        some_hist_df = some_hist_df.query('label != 0')
        tit += ", zeros excluded"
    first=some_hist_df.iloc[0,:]
    xlab=first["label_name"]
    if first["transform"] != "none":
        xlab = "%s(%s)"%(first["transform"],xlab)
    gg_hist = p9.ggplot()+\
        p9.ggtitle(tit)+\
        p9.geom_histogram(
            p9.aes(
                x="label"
            ),
            bins=30,
            color="black",
            data=some_hist_df)+\
        p9.xlab(xlab)
    return show(gg_hist)
show_data_trans("air_foil","none")
```

---

```{python}
show_data_trans("air_foil","norm01")
```

---

```{python}
show_data_trans("air_foil","log")
```

---

# forest fires data

```{python results=TRUE}
pd.set_option("display.max_columns", 6)
data_dict["forest_fires"]
```

For categorical variables like month, need to ignore, or re-encode
(ordinal or one-hot encoding).

---

```{python}
show_data_trans("forest_fires","none")
```

---

```{python}
show_data_trans("forest_fires","log")
```

---

```{python}
show_data_trans("forest_fires","log,norm01")
```

---

```{python}
show_data_trans("forest_fires","log,norm01",exclude_zero=True)
```

---

# Enforcing non-negative predictions

Assume a label $y> 0$. How to make sure that we get a positive
prediction from our neural network?

Neural network predicts $f(x)$, a real number (maybe negative).

Log-normal loss: for a given label $y$, loss is $(\log[\exp
f(x)]-\log[y])^2$, which is defined for any positive predictions $\exp
f(x)>0$.

How to handle $y=0$? Binary classification then regression.

---

# Real data feature distribution

```{python}
gg_hist = p9.ggplot()+\
    p9.ggtitle("Feature distribution in air_foil data")+\
    p9.geom_histogram(
        p9.aes(
            x="degrees"
        ),
        bins=30,
        color="black",
        data=data_dict["air_foil"])
show(gg_hist)
```

---

# Real data feature distribution

```{python}
gg_zoom_out = gg_hist+\
    p9.xlim(0, 360)
show(gg_zoom_out)
```

---

# Simulated data feature distribution


```{python}
import math
np.random.seed(1)
max_deg = 360
N_sim =100
degrees=np.concatenate([
    np.random.uniform(0,max_deg,N_sim),
    [0,max_deg]
])
degrees01=degrees/max_deg
radians=2*math.pi*degrees01
sin_vec = np.sin(radians)
cos_vec = np.cos(radians)
true = (2*cos_vec**2 + sin_vec*3.5 + 5.0 )/10
label = true+ np.random.normal(scale=0.1,size=len(true))
missing=50
set_name = np.where(
    (degrees<missing) | (degrees>max_deg-missing), "test", "train")
sim_df = pd.DataFrame({
    "degrees":degrees,
    "degrees01":degrees01,
    "radians":radians,
    "sin":sin_vec,
    "cos":cos_vec,
    "true":true,
    "label":label,
    "set_name":set_name
})
sim_df.to_csv("09-regression-sim.csv")
gg_hist = p9.ggplot()+\
    p9.ggtitle("Feature distribution in simulated data")+\
    p9.geom_histogram(
        p9.aes(
            x="degrees"
        ),
        bins=30,
        color="black",
        data=sim_df)
show(gg_hist)
```

---

# Pattern in simulated data has continuity over 0/360 edge

```{python}
gg_scatter = p9.ggplot()+\
    p9.geom_line(
        p9.aes(
            x="degrees",
            y="true"
        ),
        data=sim_df)+\
    p9.geom_point(
        p9.aes(
            x="degrees",
            y="label",
            fill="set_name"
        ),
        data=sim_df)+\
    p9.ylab("output/label")+\
    p9.xlab("input/feature (degrees)")
show(gg_scatter)
```

---

# Non-linear basis expansion

```{python}
gg_scatter = p9.ggplot()+\
    p9.geom_point(
        p9.aes(
            x="sin",
            y="cos",
            color="set_name",
            fill="label"
        ),
        size=5,
        data=sim_df)+\
    p9.coord_equal()+\
    p9.scale_fill_gradient(low="white",high="black")
show(gg_scatter)
```

---

# Nearest neighbors, baseline/code

* In binary classification, we predict the most frequent class among
  the K nearest neighbors (K=N is the featureless baseline).
* In regression we predict mean label of K nearest neighbors (instead
  of most frequent label / mode).
* That is the only difference between KNeighborsRegressor and
  KNeighborsClassifier in sklearn.neighbors.

---

# Train nearest neighbor regression 

```{python}
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV
train_df=sim_df.query("set_name=='train'")
train_nrow, train_ncol = train_df.shape
train_df.set_name = np.tile(["subtrain","validation"],train_nrow)[:train_nrow]
feature_dict = {
    "degrees":["degrees"],
    "sin,cos":["sin","cos"],
    }
true_pred_df_list = []
test_error_df_list= []
valid_error_df_list=[]
for feature_name,feature_list in feature_dict.items():
    sim_X=sim_df.loc[:,feature_list]
    train_X = train_df.loc[:,feature_list]
    learner=GridSearchCV(
        KNeighborsRegressor(), 
        {"n_neighbors":[k+1 for k in range(30)]},
        return_train_score=True)
    learner.fit(train_X, train_df.label)
    valid_df = pd.DataFrame(learner.cv_results_)
    valid_df["feature_name"]=feature_name
    sets=[
        ("train","subtrain"),
        ("test","validation"),
        ]
    for sk_name, my_name in sets:
        valid_error_df_list.append(pd.DataFrame({
            "feature_name":feature_name,
            "n_neighbors":valid_df.param_n_neighbors.astype("int"),
            "loss":1-valid_df[f"mean_{sk_name}_score"],
            "set_name":my_name
        }))
    sim_df["predicted"]=learner.predict(sim_X)
    test_df=sim_df.query("set_name=='test'")
    squared_error=((test_df.label-test_df.predicted)**2).sum()
    test_error_df_list.append(pd.DataFrame({
        "x":[0],
        "y":[0],
        "feature_name":[feature_name],
        "squared_error":[squared_error],
        "label":"squared error = %.4f"%squared_error
    }))
    for fun in "true","predicted":
        true_pred_df_list.append(pd.DataFrame({
            "feature_name":feature_name,
            "degrees":sim_df.degrees,
            "function":fun,
            "value":sim_df[fun]
        }))
true_pred_df=pd.concat(true_pred_df_list)
test_error_df=pd.concat(test_error_df_list)
valid_error_df=pd.concat(valid_error_df_list)
min_df = valid_error_df.groupby(
    ["feature_name","set_name"]
).apply(lambda DF: DF.iloc[DF.loss.argmin(),:])
gg_valid=p9.ggplot()+\
    p9.geom_line(
        p9.aes(
            x="n_neighbors",
            y="loss",
            color="set_name"
        ),
        data=valid_error_df)+\
    p9.geom_point(
        p9.aes(
            x="n_neighbors",
            y="loss",
            color="set_name"
        ),
        data=min_df)+\
    p9.geom_text(
        p9.aes(
            x="n_neighbors",
            y="loss",
            label="n_neighbors"
        ),
        data=min_df)+\
    p9.facet_grid(". ~ feature_name", labeller="label_both")
show(gg_valid)
```

---

# Learned function not continuous over 0/360

```{python}
def show_fun(feat):
    some_df=true_pred_df.query("feature_name=='%s'"%feat)
    gg_pred=p9.ggplot()+\
        p9.ggtitle("KNN Train features: "+feat)+\
        p9.geom_text(
            p9.aes(
                x="x",
                y="y",
                label="label"
            ),
            ha='left',
            data=test_error_df.query("feature_name=='%s'"%feat)
        )+\
        p9.geom_point(
            p9.aes(
                x="degrees",
                y="label",
                fill="set_name",
            ),
            data=sim_df)+\
        p9.scale_color_manual(values={
            "true":"black",
            "predicted":"violet"
        })+\
        p9.geom_line(
            p9.aes(
                x="degrees",
                y="value",
                color="function"
            ),
            size=1,
            alpha=0.75,
            data=some_df)+\
        p9.ylab("output/label")+\
        p9.xlab("input/feature (degrees)")        
    return show(gg_pred)
show_fun("degrees")
```

---

# sin/cos features enforce continuity

```{python}
show_fun("sin,cos")
```

---

# How are the neural network weights learned?

- Typically we use some version of gradient descent.
- This algorithm requires definition of a differentiable loss function
  to minimize on the train set.
- For regression problems ($y\in\mathbb R$) we use the square loss,
  $\ell[f(\mathbf x), y) = [f(\mathbf x)-y]^2$.
  
```{python}
pred_lim = 5
pred_grid = np.linspace(-pred_lim, pred_lim)
fun_dict = {
    "logistic":lambda f, y: np.log(1+np.exp(-y*f)),
    "square":lambda f, y: (y-f)**2,
}
def gg_loss(fun_name):
    fun = fun_dict[fun_name]
    loss_df = pd.concat([
        pd.DataFrame({
            "loss":fun(pred_grid, y),
            "predicted_score":pred_grid,
            "label":y,
        }) for y in (-1,1)])
    gg = p9.ggplot()+\
        p9.facet_grid(". ~ label", labeller="label_both")+\
        p9.theme_bw()+\
        p9.theme(subplots_adjust={'right': 1, 'bottom':0.2})+\
        p9.theme(figure_size=(4.5,2))+\
        p9.geom_line(
            p9.aes(
                x="predicted_score",
                y="loss",
                ),
            data=loss_df)+\
        p9.scale_x_continuous(breaks=range(-pred_lim, pred_lim+1))
    return gg
show(gg_loss("square"))
```

---

# Visualization of square loss gradient/derivative

```{python}
def log_deriv(pred, label):
    return -label / (1+np.exp(pred*label))
deriv_dict = {
    "logistic":log_deriv,
    "square":lambda f, y: 2*(f-y),
}
def gg_deriv(fun_name):
    deriv = deriv_dict[fun_name]
    loss_df = pd.concat([
        pd.DataFrame({
            "derivative":deriv(pred_grid, y),
            "predicted_score":pred_grid,
            "label":y,
        }) for y in (-1,1)])
    hline_dt = pd.DataFrame({"derivative":0},index=[0])
    gg = p9.ggplot()+\
        p9.facet_grid(". ~ label", labeller="label_both")+\
        p9.theme_bw()+\
        p9.theme(subplots_adjust={'right': 1, 'bottom':0.2})+\
        p9.theme(figure_size=(4.5,2))+\
        p9.geom_hline(
            p9.aes(
                yintercept="derivative",
            ),
            color="grey",
            data=hline_dt)+\
        p9.geom_line(
            p9.aes(
                x="predicted_score",
                y="derivative",
                ),
            size=1,
            data=loss_df)+\
        p9.scale_x_continuous(breaks=range(-pred_lim, pred_lim+1))
    return gg
show(gg_deriv("square"))

```

---

# Interactive visualization of gradient descent for regression

http://ml.nau.edu/viz/2022-02-02-gradient-descent-regression/

![](04-gradient-descent-regression.png){width=100%}

- Step size too large: subtrain loss increases (but should always
  decrease).
- Step size too small: learning is very slow (requires a lot of
  iterations to minimize loss).

---

# TODO slides

* Comparison with ReLU act in last layer to force non-negative
  predictions (including zeros).

---

# Possible exam questions

- Say $x=[5,-3,10]$, $w=[2,3,1]$, $y=6$, and we are doing regression
  (square loss). Compute loss $L$, gradient $\nabla_w L$, and new
  weights after one step of gradient descent with learning rate / step
  size = 0.1.
